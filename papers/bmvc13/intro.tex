
%-------------------------------------------------------------------------
\section{Introduction}

%1)  egocentric activity is useful for various reasons (memory, but also name 2-3 others).  recent research looking at variety of things in the space [x,y,z,a,b]
Egocentric computer vision entails analyzing images and video that originate from a wearable camera, which is typically mounted on the head or chest.  Seeing the world from this first-person point of view affords a variety of exciting new applications and challenges, particularly as today's devices become increasingly lightweight and power efficient.  For example, in the life-logging setting, a user constantly captures his daily activity, perhaps to share it with others, or to personally review it as a memory aid~\cite{Hodges2011}.  Daily logs from a wearable camera also have compelling applications for law enforcement and defense, where an archive of the first-person point of view may contain valuable forensic data.  Furthermore, in augmented reality applications, a user could be shown on an associated display (e.g., Google Glass) valuable meta-data about the objects or events he observes in real time, such as product reviews for an object he handles in the store.  Egocentric video analysis also has potential to determine how well a person can complete physical daily living tasks, thereby enabling new forms of tele-rehabilitation~\cite{Kopp97,Ramanan12}.

Nearly all such applications demand robust methods to recognize activities and events as seen from the camera wearer's perspective.  Whereas activity analysis in the traditional ``third person'' view is often driven by human body pose, in egocentric video activities are largely defined by the \emph{objects} that the camera wearer interacts with.  Accordingly, high-level representations based on detected objects are a promising way to encode video clips when learning egocentric activities~\cite{ren-gu-cvpr2010,Fathi11,Fathi-ICCV2011,Ramanan12}.  In particular, recent work explores a ``bag-of-objects'' histogram of all objects detected in a video sequence, as well as a spatio-temporal pyramid extension that captures the objects' relative temporal ordering~\cite{Ramanan12}.  Coupled with standard discriminative classifiers, this representation shows very good results; notably, it outperforms histograms of local space-time visual words, a favored descriptor in current third-person activity recognition systems.

%2) problem: ego activities defined by objects, yet how to bin / preserve object relationships loosely?
% existing work does hand coding. cite Ramanan, Choi, and Laptev as a group here; It is a problem to hand code because...
However, existing methods that pool localized  visual features into space-time histogram bins do so using hand-crafted binning schemes, whether applied to egocentric video or otherwise.  For example, the spatial pyramid widely used for image classification~\cite{Lazebnik06} is extended to space-time in~\cite{Choi08,Ramanan12}, using a hierarchy of regularly sized volumetric bins to pool the detected features at different granularities.  In~\cite{Laptev08}, a series of coarse partitions are defined (dividing the video into thirds top to bottom, etc.), then aggregated by summing kernels.  The problem with defining the spatio-temporal bins \emph{a priori} is that they may not offer the most discriminative representation for the activity classes of interest.  That is, the hand-crafted histogram bins may fail to capture those space-time relationships between the component objects (or other local features) that are most informative.


%3) our idea: discriminatively learn the partitions and also efficiently focus the candidate partitions by using object-centric partition sampling...
To overcome this limitation, we propose to \emph{learn} discriminative spatio-temporal histogram partitions for egocentric activities.  Rather than manually define the bin structure, we devise a boosting approach that automatically selects a small set of useful spatio-temporal pyramid histograms among a randomized pool of candidate partitions.  In this way, we identify those partitions that most effectively pool the detected features (in this case, the detected objects).  Since training time for boosting grows linearly with the number of candidates, relying on purely random space-time cuts can be computationally expensive.  Therefore, we further propose a way to meaningfully bias the partitions that comprise the candidate pool.  We devise an \emph{object-centric} cutting scheme that prefers sampling bin boundaries near objects involved in the egocentric activities. In particular, our method is more likely to sample partitions that cut through video regions containing ``active'' objects~\cite{Ramanan12} (e.g., the open microwave, the pot handled on the stove), thereby concentrating layout information on the key interactions.  As a result, we focus the randomized pool of space-time partitions to the egocentric setting while also improving training efficiency.

%
%%4) method overview - mechanics (short para, mentions partition generation and boosting)
%We implement our idea as follows.  Given a set of egocentric training videos labeled according to their activity class, we first run object detectors on the frames to localize any objects of interest---both those that are ``passive" and those that are ``active" in an interaction with the camera wearer.  Each detected object has a space-time location $(x,y,t)$, the center of its bounding box.  We then construct a series of candidate space-time pyramids, in which each axis-aligned bin boundary is translated by some random shift.  The random shifts are non-uniform; they are sampled using the distribution of all active object coordinates in the training data.  Given this candidate pool of pyramids, we compute the corresponding series of object histograms for each training video, where a detected object is counted in the space-time bin its center occupies.  Then, we apply multi-class boosting to select a subset of discriminative pyramid structures based on how well they can be used to classify the activities of interest.  At the end, we have a strong classifier that can predict the labels of new videos, using only those randomized pyramids selected by the learning algorithm.


%5) results teaser statement about what will be shown
We apply our method to the challenging Activities of Daily Living dataset, and show that the proposed method improves the state of the art.  The results show the value of learning discriminative space-time partitions, compared to both bag-of-words or existing spatio-temporal pyramids.  Furthermore, we demonstrate the key role played by object-centric cuts in terms of focusing the candidate pyramids.

%; our method finds useful partitions more quickly than a simpler alternative that applies boosting to randomized pyramids with uniformly sampled shifts.
