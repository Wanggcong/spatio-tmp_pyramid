\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{algpseudocode}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Biased Randomized Spatio-Temporal Pyramids for Egocentric Activity Recognition}

\author{Tomas McCandless and Kristen Grauman\\
University of Texas at Austin\\
{\tt\small \{tomas, grauman\}@cs.utexas.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
	Egocentric video and wearable computing have become increasingly
	prevalent in the past decade, resulting in a huge explosion in the amount
	of available video content. In this paper, we present a novel approach for
	egocentric activity recognition using the UC Irvine ADL (Activities of Daily Living)
	dataset \cite{Ramanan12}.  
  Existing work in activity recognition uses predefined binning schemes,
  which may fail to capture important temporal relationships between
  features.
  We propose to partition video clips into sets of
	3-dimensional cuboids based on many different multi-level randomized partitioning
	schemes, then concatenate object histograms
	over multiple levels to form feature vectors which we then use to train a pool
	of weak SVM classifiers. 
	Finally, we use a boosting algorithm to learn which partitioning schemes are
  most discriminative and form a
	final strong classifier with accuracy that improves upon the current state of
	the art. Our main novel contribution is a method for
	creating biased partition schemes based on observed distributions of
	active object locations across each spatial and temporal dimension of the video clips.
  We found that partitions which cut through spatio-temporal regions that
  tend to contain active objects are often more discriminative than
  unbiased partitions and 
  partitions that cut around such active object regions.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
	Activity recognition is becoming an increasingly canonical problem in
	computer vision as researchers are beginning to explore the domain more
  thoroughly and several relevant datasets have been released \cite{Schuldt04, Rodriguez08}. The problem 
	of human activity recognition is in some ways less well defined
	than, say, object recognition for 2D images, in part due to the relative
	lack of datasets for activity recognition, and also because it is somewhat
	problematic to define a canonical representation for each type of action.
	In other words, it seems as though there can be higher intra-class
	variation for activity recognition than for object recognition. 
	% past datasets scripted activities
	Datasets geared towards activity recognition in the past have often
	consisted of actors performing scripted activities in a static and at
  times artificial environment \cite{Schuldt04}. However, in order to develop robust and effective
	recognition methods, we need datasets that are more organic in the sense that they
	depict unscripted activities occurring in a natural environment such as a home or
	apartment.
	\cite{Ramanan12}. 
	% similarity of problems, occlusion, clutter
	However, activity recognition and object recognition do share some
	similar properties. For instance, similar learning techniques are applicable to both
  scenarios, and occlusion and background clutter are
	difficulties that arise in both problems.

	% application to life logging
	A robust and accurate method for egocentric activity recognition would have 
	many useful practical applications. For instance,
	a recent trend in wearable computing is so-called life logging which can
	assist patients suffering from memory loss \cite{Sellen07}. However, with
	the storage of large amounts of video, it becomes necessary to have a system for
	efficiently browsing video. A robust egocentric activity recognition
	system could automatically tag video clips with types of activities,
	thus allowing the user to, for
	instance, quickly find all clips recorded in the past that depict making tea.

	% application to senior living
	There are many clinical benchmarks used to evaluate patients everyday
	functional abilities \cite{Kopp97, Catz97, Itzkovich07}. 
	These benchmarks are currently conducted in a
	hospital setting, but a robust system for egocentric activity recognition
	could greatly impact the workflow for patient evaluation, as such a system
	would allow for passive long term observation of patients in their own
	homes. This could lead to more accurate clinical evaluations since it would be
	possible to collect far more data about individual patients in the form of continuous 
  monitoring rather than infrequently benchmarked. Such a system
	would also eliminate the need for patients to commute to a hospital to have
	evaluations done, thus reducing cognitive and physical burden on patients.
	
	% TODO application to law enforcement?
  Egocentric activity recognition in a daily living context differs from
  non-egocentric activity recognition because activities can have long-term
  temporal dependencies and actions can be interrupted by other actions. For
  example, a user might wash a few dishes while waiting for a cup of tea
  to brew. The familiar bag-of-words approach can be used to aggregate space-time
  features with reasonable performance,  but falls short because it
  fails to capture temporal dependencies between features.

  The pyramid is a well-known extension of a pure bag-of-words model that encodes spatial
  relationships between features by recursively subdividing images or video and extracting 
  features from each spatial bin \cite{Lazebnik06}, yielding impressive
  results across a range of applications.

  Previous work in egocentric activity recognition has employed a single strict
  hand-coded partition scheme \cite{Ramanan12}, which may not be particularly robust to
  inter and intra-class variation. The work of \cite{Laptev08} uses multiple
  candidate spatio-temporal grids for the task of activity recognition (but
  not in an egocentric setting), however each grid is predefined and only 24
  candidate grids are considered. With such a small pool of schemes for imposing spatial
  information, the most discriminative space-time relationships between features may not be 
  captured. The work presented in \cite{Kovashka10}
  describes an effective state-of-the-art method for learning the shapes of spatio-temporal
  regions on a per-class basis, but makes use of lower-level features such as optical flow and
  is not applied in an egocentric setting.

  Spatial pooling of features in a learned way for object recognition in 2D images
  has been thoroughly
  explored \cite{Sharma11}, but to our knowledge there has been little work
  on learning the best way to pool spatio-temporal features.
  
  Our method, however, builds on existing work by creating a large number
  of candidate partitioning schemes in a randomized way. Our main novel contribution is
  the ability to bias this randomization step so that partitioning schemes in the
  resulting pool have a high probability of cutting through or around
  spatio-temporal regions which tend to contain active objects.
  We then pool spatio-temporal features in a
  learned way, selecting those partitioning schemes which are most discriminative.
  We found that partitioning schemes that cut through regions known to contain active
  objects are often the most discriminative.

%-------------------------------------------------------------------------
\subsection{Related Work}


	In \cite{Laptev08}, Laptev \etal investigate automatically aligning movie scripts with
	video for the purpose of annotating human actions, and achieve 91.8\%
  accuracy on the KTH dataset. The method presented in \cite{Laptev08} uses a
  relatively small number of hard-coded schemes for spatio-temporal binning,
  which may fail to capture important spatio-temporal relationships between
  features.
  In
	particular, Laptev \etal show how to learn relevant scene classes along
	with any correlations they may have with human activity.
	
	In \cite{Marszalek09}, Marszalek \etal released a novel dataset based on
	Hollywood movies that contains twelve types of activities and ten
	different classes of scenes. The main contribution of this paper is based
	on the observation that the visual content of a human's environment can
	impose useful constraints on the type of activity occurring. For instance,
	food preparation activities frequently occur in a kitchen environment. 
	
	In \cite{Fathi12}, Fathi \etal focus on the relationship between gaze and
	activity recognition in an egocentric setting and develop methods to
	predict activity given gaze, gaze given activity, and both
	activity and gaze given neither. The activities in this
	published dataset are primarily related to food preparation. 
	
	The main work related to our own is that carried out in \cite{Ramanan12}. 
	In this work the ADL dataset is introduced as well as detailed analysis of
	performance of several different classifiers. 
  The ADL dataset consists of hundreds of egocentric video clips
	(roughly 10 hours of video in total) collected from 20 people performing
	18 types of unscripted actions in their own homes. These unscripted
  actions are often related to hygiene or food preparation and are more
  varied than actions presented in previous datasets such as that presented
  in \cite{Fathi11}.
	\begin{figure}[t]
		\begin{center}
			%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
			  \includegraphics[width=0.8\linewidth]{/u/tomas/thesis/figures/thumbnail.jpg}
		\end{center}
    \caption{An example frame from the ADL dataset with annotations for hand
    position and detected objects} 
				\label{fig:long}
				\label{fig:onecol}
	\end{figure}
  There are 26 different 
	types of detected objects, including 5 active and 21 passive objects. 
  Composite object models are developed for the purpose of object detection. 
  These models take advantage of the fact that objects can have vastly
  different appearances when they are being interacted with. For example, a
  refrigerator or microwave has a different appearance when it is open and
  being interacted with. Object detectors are trained on videos from the
  first 6 people and tested on the videos from the remaining 14 people.
  
	\begin{table}
		\begin{center}
			\begin{tabular}{|l|c|}
				\hline \hline
        1 & combing hair \\
        \hline
        2 & make up \\
        \hline
        3 & brushing teeth \\
        \hline
        4 & dental floss \\
        \hline
        5 & washing hands/face \\
        \hline
        6 & drying hands/face \\
        \hline
        7 & laundry \\
        \hline
        8 & washing dishes \\
        \hline
        9 & moving dishes \\
        \hline
       10 & making tea \\
        \hline
       11 & making coffee \\
        \hline
       12 & drinking water/bottle \\
        \hline
       13 & drinking water/tap \\
        \hline
       14 & preparing cold food/snack \\
        \hline
       15 & vacuuming \\
        \hline
       16 & watching tv \\
        \hline
       17 & using computer \\
        \hline
       18 & using cell phone \\
				\hline\hline
			\end{tabular}
		\end{center}
		\caption{Types of activities present in the ADL dataset.}
	\end{table}
  
	Each frame in the dataset
	is annotated with activity labels and bounding boxes for detected objects and hand positions, 
	Additionally, each object is tagged as active or passive depending
	on whether it is being interacted with.
  A comparison of the well-known bag-of-words approach with a strict hard-coded
  2-level temporal pyramid using both space-time interest points and the
  results of the part-based object detectors is presented. The temporal pyramid makes 
  a single cut along the temporal dimension and no cuts along the
  spatial dimensions, but is simple, easy to implement, and outperforms a 
  classifier trained on bag-of-words histograms.
  
	The crucial contribution of
	\cite{Ramanan12} is that egocentric activity recognition is ``all about
	the objects'', particularly the objects being interacted with, as
	recognition accuracy increases dramatically when ground truth object
	locations rather than detected locations are used to train the classifier. 

	The video collected for the ADL dataset is available in a temporally
	presegmented format; each video has been segmented into clips depicting
	activities. Egocentric video is captured as a continuous stream, and thus
  a pre-processing step of temporal segmentation into discrete events is
  required. There is a large amount of literature on temporal segmentation
  of video. For instance, work presented in \cite{Lee12} includes a method for
	automatic temporal segmentation of egocentric video into events.
  
	Our algorithm is inspired by the work of \cite{Jiang12}, which uses a
  a version of the SAMME Ada-boost algorithm \cite{Zhu06}
  with randomized spatial pyramids for 2D images, 
	leading to increased robustness to intra-class variation based on results
  from benchmarks on three publicly available datasets. However, in contrast
  with our own work, the randomized pyramids are not biased in any way. 
	

	% work from other researchers on segmentation
	% work from other researchers on activity recognition with other modes of
	% sensor data

\section{Approach}

	Our boosting algorithm takes as input a collection of labeled training videos
	and a pool of candidate partition patterns. We use the output of the
  object detectors trained on composite object models as our features to be
  pooled.
  We train a separate ``weak''
  multi-class SVM 
  (using LIBSVM \cite{Chang11})
	classifier on the feature vectors resulting from representing the training
	data using each candidate partition pattern. We set a weight for each
	training point $p_i$ that is inversely proportional to the number of points
	with the same class as $p_i$. During each round of boosting we select the
	candidate partition $\theta_j$ that is most discriminative (has minimum training
	error), compute a weight for $\theta_j$, and compute accuracy for the
	current version of the final strong classifier. 
	We set the number of boosting rounds to 30, noting that additional boosting
	rounds only give a marginal boost to performance. Additionally, with a
	larger number of boosting rounds, overfitting becomes a possibility.\\

	\textbf{Algorithm 1} Training RSTP Classifier via Boosting \\
	\textbf{INPUT:} 
	\begin{itemize}
		\item $N$ labeled training videos $\Phi = \{(V_i, c_i)\}_{i=1}^N$
		\item A pool of partition patterns $\Theta = \{\theta\}$
	\end{itemize}
	\textbf{OUTPUT:}
	\begin{itemize}
		\item A strong video classifier $F$. For an unlabeled video $V$, 
			$c=F(V)$ is the predicted label for $V$.
			\begin{enumerate}

				\item For each $\theta \in \Theta$
					\begin{itemize}
						\item Train a multi-class classifier (SVM) $f_\theta$ on $\Phi$
					\end{itemize}

				\item Initialize:
					\begin{itemize}
						\item weight $w_i = \frac{1}{C N_{c_i}}$ for each video clip,
							where $N_{c_i}$ is the number of videos with label $c_i$,
              and $C$ is the number of distinct labels in the training data.
						\item current iteration number $j=0$.
						\item current accuracy $\sigma_j = 0$.
					\end{itemize}

				\item For each round of boosting:
					\begin{itemize}
						\item increment $j$.
						\item Re-normalize the weight vector: $w_i = \frac{w_i}{\Sigma_i^N w_i}$.
					  \item For each pattern $\theta$, compute its classification
							error $err_\theta$ as the dot product product of $w$ with the indicator
							vector of incorrect classifications using $f_\theta$. 
						\item Choose the pattern $\theta_j$ with minimum error $err_j$
						\item Compute the weight for $\theta_j$ as: \\
							$\alpha_j = \mbox{log} \frac{1 - err_j}{err_j} + \mbox{log}(C
							- 1)$
						\item Update the weight vector:\\
							$\mbox w_i = w_i * \mbox{exp}(\alpha_j *
							\mbox{\textbf{I}}(f_{\theta_j}(V_i) \neq c_i))$.
						\item Generate the strong classifier: \\
							$F(V) = \mbox{arg max}_c \Sigma_{m=1}^j \alpha_m *
							\mbox{\textbf{I}}(f_{\theta_m}(V) = c)$
					\end{itemize}

			\end{enumerate}
	\end{itemize}
	
	The original version of the SAMME algorithm has each weak classifier
	$f_\theta$ trained on
	a randomly selected subset of the training dataset, but we train each of
	our weak classifiers on the full training dataset in order to reduce the
	number of randomized portions of our method, making it easier to
	reason about.

  \subsection{RSTP Implementation}
	We use k-d trees to represent randomized partition schemes. Each level in the tree
	represents a set of cuts along a certain dimension, and we generate cuts
  in a round-robin manner over dimensions $(x, y, t)$ across subsequent levels in the
  tree.
  Cuts for child nodes are generated independently, and each cut is axis-aligned
  (we incorporate random shifts, but not random rotations).
	Initially, all randomized partitions were computed according to a uniform
	distribution. However, in an attempt to avoid generating partition schemes
	that are not sufficiently discriminative, we bias the partition generation
	step according to computed distributions of active object locations across
	training data. 

	\begin{figure}[t]
		\begin{center}
			%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
			  \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/active_obj_distr_x.png}
			  \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/active_obj_distr_y.png}
		\end{center}
		   \caption{Histograms of counts of active objects across the $x$ and
       $y$ spatial dimensions. Active objects tend to appear in the lower
     center field of view.}
				\label{fig:long}
				\label{fig:onecol}
	\end{figure}
	
	From figure 2 we see that active objects often tend to occur in the lower center
	of the field of view. This is as expected, because
	the active objects are close to the hands which are in the lower field of
	view from an egocentric perspective. Since different clips can have
  varying lengths with respect to time, we normalize the length of each
  video clip to 1 and consider relative temporal locations of active
  objects. The distribution of active objects
  across the temporal dimension is nearly uniform. When generating a biased
	partition scheme, we can choose to prefer splits that cut around regions that tend to
	contain active objects (denoted as bias type 2), or we can choose to prefer 
	splits that cut through regions that tend to contain active objects (denoted as bias type 3). 
	We denote by bias type 1 the method of using uniform
	distributions to generate partitions. For biased partitions, we generate
  the first split along each dimension according to a weighted distribution
  corresponding to the histograms of observed active object regions in the training data,
  and we generate all subsequent child cuts using a uniform distribution.
  We do not consider locations of passive objects at all during the
  generation of biased partition schemes. Since active objects are located
  in close spatial proximity to hands, creating biased partition schemes can
  be interpreted as implicitly taking into account information about hand
  locations.
  

  \begin{figure}[t]
    \begin{center}
		 \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/lvl3.png}
    \end{center}
		   \caption{An example biased (type 3) partitioning scheme corresponding to a 3-level
       pyramid. Splits along the lower portion of the $y$ dimension are
     visible, corresponding to the tendency for active objects to appear in
   the lower area of the field of view.}
			\label{fig:long}
			\label{fig:onecol}
  \end{figure}
	
   To represent a video clip as a randomized spatio-temporal pyramid (RSTP)
   using a particular partition scheme we use the output of object detectors
   trained in \cite{Ramanan12}, which gives bounding boxes and object
   labels for each extracted frame. We compute histograms of detected
   objects for each individual level in the pyramid,
   where level 0 is the entire video clip volume and level $i$ is all the
   cells of depth $i$ in the k-d tree. 
   Note that level $i$ has $8^i$ leaf
   cells. To form the final RSTP representation, we simply concatenate the
   histograms computed for each level to form a single feature vector.
   We can choose whether or not to include detected active objects when
   forming an RSTP representation of a video clip, however taking active
   objects into account gives a substantial improvement to overall
   classification accuracy.
   


\section{Results}
	The ADL dataset has been modified since the publication of
	\cite{Ramanan12}; because of this, running the published code gives
	slightly lower accuracy than the originally published numbers. We use the
  modified version of the dataset available from the authors webpage at the time of writing to
  benchmark our method. One difficulty that can arise within egocentric
  activity recognition is that activities can be temporarily interrupted by
  other activities. For instance, while waiting for tea to brew a subject
  may watch TV. For cases of such interruptions, to avoid unnecessary
  complications resulting from frames being annotated with multiple
  activities, the ADL dataset simply uses the label of the interrupting
  action when a longer action is disrupted.
	
	\begin{table}
		\begin{center}
			\begin{tabular}{|l|c|c|c|c|}
				\hline
        Feature Type & BoW & Temporal Pyramid & RSTP \\
				\hline\hline
        STIP & 16.5\% & 22.8\% & - \\ 
        \hline
       O & 26.6\% & 29.0\% & 32.7\%\\
        \hline
      AO & 34.9\% & 36.9\% & 38.7\%\\
				\hline
			\end{tabular}
		\end{center}
		\caption{Overall classification accuracy on pre-segmented video clips.}
	\end{table}
	
  Table 2 shows a comparison of overall classification accuracy between our
  method and two methods presented in \cite{Ramanan12}. The temporal pyramid
  has two levels, formed by making a single cut along the temporal
  dimension and no cuts along the spatial dimensions.
  
  Row 1 shows results originally published in \cite{Ramanan12} which we
  reproduce to illustrate the advantage of using higher level features such
  as detected objects over low level features such as space-time interest
  points.  Row 2 shows results
  obtained using only passive detected objects, while row 3 shows results obtained
  using both active (being interacted with) and passive detected objects.
  The consideration of active objects when constructing feature vectors
  gives a significant improvement over just including passive objects, and
  in both cases our method improves on the current state of the art.

	The results shown in Table 2 are computed using a form of cross
	validation (the video clips from person $i$ are used as a held out validation set, and
	training occurs using the video clips from the remaining people).
  Following \cite{Ramanan12}, we exclude videos from the first 6 people
  (because they were used to train the object detectors) from our
  experiments.
  
  Feature vectors are computed using detections for both active and passive
  objects. The results for bag of words and temporal pyramids (2 level, with
  a single cut along the temporal dimension) are both presented in
  \cite{Ramanan12}.
  
  For this experiment we used pools of 4-level partitioning schemes of
  varying sizes with a varying number of boosting rounds. The numbers
  presented in table 1 were obtained with 5 boosting rounds and a pool of
  size 70.
  Each partitioning scheme was of bias type 3, meaning the cuts for level 1
  were biased such that they had a tendency to cut through regions containing
  active objects, and the cuts for levels 2 and 3 were drawn from a uniform
  distribution. The work of \cite{Jiang12}, which uses a similar
  pyramid-based boosting approach for 2D image recognition, found that using
  pyramids with more than 3 levels actually led to a decrease in overall
  accuracy due to over-segmentation of image space. However, we found that in the
  3D case 4-level pyramids give better overall accuracy than coarser-grained
  representations.
	
  \begin{figure}[t]
		\begin{center}
			%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
			  \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/confn.png}
		\end{center}
		   \caption{Confusion matrix for our method using active objects}
				\label{fig:long}
				\label{fig:onecol}
	\end{figure}
	As seen in figure 4, our method has particularly good
  performance for activity types 5 and 6 (``combing hair'' and ``drying
  hands/face'', respectively). Some activity types on which our method does
  poorly are 10 and 11, which are ``making tea'' and ``making coffee'',
  respectively (see Table 1 for a full listing of activity types present in
  the ADL dataset). Since the two activity types are similar in the sense that
  they involve the same active objects, it is not
  unexpected that a recognition system would confuse them often.
  
  To illustrate the improvement on accuracy obtained from using a pool of
  biased partitions, we created 3 separate pools containing 4-level
  partition schemes of each bias type and
  repeatedly ran the cross-validation experiment, adding additional
  partitions to each pool between runs. The pool containing partitions of
  bias type 3 consistently outperformed the pool of bias type 2, which
  consistently outperformed the pool of unbiased partition schemes. The
  results from this experiment are depicted in figure 5. We were initially
  surprised to find that adding more partition schemes to pools can
  sometimes negatively impact overall classification accuracy, however we
  believe this is due to the inherent bias between the training and the test
  data. In other words, sometimes a partition scheme with low training error
  on the train data that gets selected during a round of boosting can have a
  larger training error on the test data. A similar behavior was discovered
  in the case of boosting spatial pyramids for 2D image recognition
  \cite{Jiang12}.
 
  \begin{figure}[t]
		\begin{center}
			%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
			  \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/biaseffect.png}
		\end{center}
		   \caption{Effect of using biased partition schemes}
				\label{fig:long}
				\label{fig:onecol}
	\end{figure}
  
  To further support our claim that 4-level pyramids of bias type 3 tend to be most
  discriminative, we created a heterogeneous pool containing partition
  schemes of
  several different types. Specifically, the heterogeneous pool contains 30 2-level
  partitions of each bias type, 30 3-level partitions of each bias type, and
  30 4-level partitions of each bias type, for a total of 270 partitions.
  We generated 20 random 50/50 train/test splits, fixed the number of
  boosting rounds to 5, and observed which types of partitions were most
  often selected during boosting rounds.
  We found that partition schemes with more levels tend to get selected more
  often during boosting rounds.
  Specifically, we found that 2-level pyramids of bias type 3 were selected
  21\% of the time, 3-level pyramids of bias type 3 were selected 19\% of
  the time, and 4-level pyramids of bias type 3 were selected 37\% of the
  time. 2-level and 4-level unbiased pyramids were never selected. Thus,
  biased partition schemes that cut through regions that tend to contain
  active objects are clearly more discriminative than other types of
  partition schemes, especially those which are unbiased.
	
\section{Conclusion and Future Work}
	We have presented an application of the well-known boosting framework
	with results that improve upon the current state of the art. 
	Our main novel contribution is a method for generating biased partition
	schemes based on observations of active object locations throughout the
  training data.
	Future work could incorporate different types of biases when generating
	partitions. The ADL dataset also includes annotations for hand positions,
	which we have incorporated implicitly through our generation of partitions
	biased relative to regions which tend to contain active objects. However,
	it could be possible to incorporate explicit information given by hand
	positions to obtain better classification results.
	Additionally, it may be worthwhile to investigate the performance of other
	variants of the boosting algorithm.
	The partitions we focus on contain cuts that are
  planar and axis-aligned, but it is also possible to carve up the
	video volume in non-linear ways. Such a method would involve more
	sophisticated computational geometry, but may yield a more discriminative
	partitioning scheme that could lead to better classification accuracy.
  

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
