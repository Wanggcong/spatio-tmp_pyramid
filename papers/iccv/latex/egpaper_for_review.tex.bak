\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{algpseudocode}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Randomized Spatio-Temporal Pyramids for Egocentric Activity Recognition}

\author{Tomas McCandless and Kristen Grauman\\
University of Texas at Austin\\
{\tt\small \{tomas, grauman\}@cs.utexas.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
	Egocentric video and wearable computing have become increasingly
	prevalent in the past decade, resulting in a huge explosion in the amount
	of video content. In this paper, we present a novel approach for
	egocentric activity recognition using the UC Irvine ADL (Activities of Daily Living)
	dataset \cite{Ramanan12}.  
  Existing work in activity recognition uses predefined binning schemes,
  which may fail to capture important temporal relationships between
  features.
  The method we present partitions video clips into
	3-dimensional cuboids, based on many different multi-level randomized partitioning
	schemes, then concatenates object histograms
	over multiple levels to form feature vectors which we use to train a pool
	of weak SVM classifiers. 
	Finally, we use a boosting algorithm to learn the most discriminative
	partitions and form a
	final strong classifier with accuracy that improves upon the current state of
	the art. Our main novel contribution is a method for
	creating biased partition schemes based on observed distributions of
	active object locations across each dimension of the dataset.
  We found that partitions which cut through spatio-temporal regions that
  tend to contain active objects are often more discriminative than
  unbiased partitions and 
  partitions that cut around such active object regions.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
	Activity recognition is becoming an increasingly canonical problem in
	computer vision as researchers are beginning to explore the domain more
	thoroughly and several relevant datasets have been released. The problem 
	of human activity recognition is in some ways less well defined
	than, say, object recognition for 2D images, in part due to the relative
	lack of datasets for activity recognition, and also because it is somewhat
	problematic to define a canonical representation for each type of action.
	In other words, it seems as though there can be higher intra-class
	variation for activity recognition than for, say, object recognition. 
	% past datasets scripted activities
	Datasets geared towards activity recognition in the past have often
	consisted of actors performing scripted activities in a static and at
	times artificial environment, yet in order to develop robust and effective
	methods, we need datasets that are more organic in the sense that they
	depict unscripted activities in a natural environment such as a home or
	apartment.
	\cite{Ramanan12}. 
	% similarity of problems, occlusion, clutter
	However, activity recognition and object recognition do share some
	similar properties. For instance, occlusion and background clutter are
	problems that arise in both problems.

	% application to life logging
	A robust and accurate method for egocentric activity recognition would have 
	many practical applications. For instance,
	a recent trend in wearable computing is so-called life logging which can
	assist patients suffering from memory loss \cite{Sellen07}. However, with
	such large amounts of video, it becomes necessary to have a system for
	efficiently browsing video. A robust egocentric activity recognition
	system could automatically tag video clips with types of activities (this
	could be done either online or offline), thus allowing the user to, for
	instance, quickly find all clips in the past that depict making tea.

	% application to senior living
	There are many clinical benchmarks used to evaluate patients everyday
	functional abilities \cite{Kopp97, Catz97, Itzkovich07}. 
	These benchmarks are currently conducted in a
	hospital setting, but a robust system for egocentric activity recognition
	could greatly impact the workflow for patient evaluation, as such a system
	would allow for passive long term observation of patients in their own
	homes. This could lead to more accurate evaluations since it would be
	possible to collect far more data about individual patients. Such a system
	would also eliminate the need for patients to commute to a hospital to have
	evaluations done, thus reducing cognitive and physical burden on patients.
	
	% TODO application to law enforcement?

  Previous work in activity recognition has employed a single strict
  hand-coded partition scheme \cite{Ramanan12}, which may not be particularly robust to
  inter and intra-class variation. The work of \cite{Laptev08} uses multiple
  candidate spatio-temporal grids for the task of activity recognition (but
  not in an egocentric setting), however each grid is hand-coded and only 24
  candidate grids are considered. The work presented in \cite{Kovashka10}
  describes an effective method for learning the shapes of spatio-temporal
  regions on a per-class basis, but makes use of lower-level features and
  is not applied in an egocentric setting.

  Spatial pooling of features in a learned way has been thoroughly
  explored \cite{Sharma11}, but to our knowledge there has been little work
  on learning the best way to pool spatio-temporal features.
  
  Our method, however, builds on existing work by creating a larger number
  of candidate partitioning schemes in a randomized way. Our main novel contribution is
  the ability to bias this randomization step so that partitions in the
  resulting pool have a high probability of cutting through or around
  spatio-temporal regions which tend to contain active objects.
  We then pool spatio-temporal features in a
  learned way, selecting those partitioning schemes which are most discriminative.

%-------------------------------------------------------------------------
\subsection{Related Work}


	In \cite{Laptev08}, Laptev \etal investigate aligning movie scripts with
	video for the purpose of annotating human actions, and achieve 91.8\%
	accuracy on the KTH dataset. The method presented in this paper uses a
  relatively small number of hard-coded schemes for spatio-temporal binning,
  which may fail to capture important spatio-temporal relationships between
  features.
	
	In \cite{Marszalek09}, Marszalek \etal released a novel dataset based on
	Hollywood movies that contains twelve types of activities and ten
	different classes of scenes. The main contribution of this paper is based
	on the observation that the visual content of a human's environment can
	impose useful constraints on the type of activity occurring. For instance,
	food preparation activities frequently occur in a kitchen environment. In
	particular, Laptev \etal show how to learn relevant scene classes along
	with any correlations they may have with human activity.
	
	In \cite{Fathi12}, Fathi \etal focus on the relationship between gaze and
	activity recognition in an egocentric settting and develop methods to
	predict activity given gaze, gaze given activity, and to predict both
	activity and gaze. The activities in this
	published dataset are primarily related to food preparation. 
	
	The main work related to our own is that carried out in \cite{Ramanan12}. 
	In this work the ADL dataset is introduced as well as detailed analysis of
	performance of several different classifiers. 

  
  The ADL dataset consists of hundreds of egocentric video clips
	(roughly 10 hours of video in total) collected from 20 people performing
	18 types of unscripted actions in their own homes. These unscripted
  actions are often related to hygeine or food preparation and are more
  varied than actions presented in previous datasets such as that presented
  in \cite{Fathi11}.
  There are 26 different 
	types of detected objects, including 5 active and 21 passive objects. 
	Each frame in the dataset
	is annotated with activity labels and bounding boxes for detected objects and hand positions, 
	Additionally, each object is tagged as active or passive depending
	on whether it is being interacted with.
  A comparison of the well known bag-of-words approach with a strict hard-coded
  2-level temporal pyramid is presented. The temporal pyramid makes no cuts along the
  spatial dimensions, but is easy to implement, simple, and outperforms a 
  classifier trained on bag-of-words histograms.
	The crucial contribution of
	\cite{Ramanan12} is that egocentric activity recognition is ``all about
	the objects'', particularly the objects being iteracted with, as
	recognition accuracy increases dramatically when ground truth object
	locations rather than detected locations are used to train the classifier. 

	\begin{figure}[t]
		\begin{center}
			%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
			  \includegraphics[width=0.8\linewidth]{/u/tomas/thesis/figures/thumbnail.jpg}
		\end{center}
		   \caption{An example frame with annotations TODO: find a better image?}
				\label{fig:long}
				\label{fig:onecol}
	\end{figure}
	
	Our algorithm is inspired by the work of \cite{Jiang12}, which uses a
  a version of the SAMME Ada-boost algorithm \cite{Zhu06}
  with randomized spatial pyramids for 2D images, 
	leading to increased robustness to intra-class variation. However, the
  randomized pyramids are not biased in any way. The method
  introduced by \cite{Jiang12} is benchmarked on three public datasets.
	
	The video collected for the ADL dataset is available in a temporally
	presegmented format; the shots have been segmented into clips depicting
	activities. The work presented in \cite{Lee12} includes a method for
	temporally segmenting egocentric video into events.

	% yong jae segmentation
	% work from other researchers on segmentation
	% work from other researchers on activity recognition with other modes of
	% sensor data

\section{Approach}

	Our boosting algorithm takes as input a collection of labeled training videos
	and a pool of candidate partition patterns. We train a separate weak SVM 
  (using LIBSVM \cite{Chang11})
	classifier on the feature vectors resulting from representing the training
	data using each candidate partition pattern. We set a weight for each
	training point $p_i$ that is inversely proportional to the number of points
	with the same class as $p_i$. During each round of boosting we select the
	candidate partition $\theta_j$ that is most discriminative (has minimum training
	error), compute a weight for $\theta_j$, and compute accuracy for the
	current version of the final strong classifier. 
	We set the number of boosting rounds to 30, noting that additional boosting
	rounds only give a marginal boost to performance. Additionally, with a
	larger number of boosting rounds, overfitting becomes a possibility.\\

	\textbf{Algorithm 1} Training RSTP Classifier via Boosting \\
	\textbf{INPUT:} 
	\begin{itemize}
		\item $N$ labeled training videos $\Phi = \{(V_i, c_i)\}_{i=1}^N$
		\item A pool of partition patterns $\Theta = \{\theta\}$
	\end{itemize}
	\textbf{OUTPUT:}
	\begin{itemize}
		\item A strong video classifier $F$. For an unlabeled video $V$, 
			$c=F(V)$ is the predicted label for $V$.
			\begin{enumerate}

				\item For each $\theta \in \Theta$
					\begin{itemize}
						\item Train a multi-class classifier (SVM) $f_\theta$ on $\Phi$
					\end{itemize}

				\item Initialize:
					\begin{itemize}
						\item weight $w_i = \frac{1}{C N_{c_i}}$ for each video clip,
							where $N_{c_i}$ is the number of videos with label $c_i$.
						\item current iteration number $j=0$.
						\item current accuracy $\sigma_j = 0$.
					\end{itemize}

				\item For each round of boosting:
					\begin{itemize}
						\item increment $j$.
						\item Re-normalize the weight vector: $w_i = \frac{w_i}{\Sigma_i^N w_i}$.
					  \item For each pattern $\theta$, compute its classification
							error $err_\theta$ as the dot product product of $w$ with the indicator
							vector of incorrect classifications using $f_\theta$. 
						\item Choose the pattern $\theta_j$ with minimum error $err_j$
						\item Compute the weight for $\theta_j$ as: \\
							$\alpha_j = \mbox{log} \frac{1 - err_j}{err_j} + \mbox{log}(C
							- 1)$
						\item Update the weight vector:\\
							$\mbox w_i = w_i * \mbox{exp}(\alpha_j *
							\mbox{\textbf{I}}(f_{\theta_j}(V_i) \neq c_i))$.
						\item Generate the strong classifier: \\
							$F(V) = \mbox{arg max}_c \Sigma_{m=1}^j \alpha_m *
							\mbox{\textbf{I}}(f_{\theta_m}(V) = c)$
					\end{itemize}

			\end{enumerate}
	\end{itemize}
	
	The original version of the SAMME algorithm has each weak classifier
	$f_\theta$ trained on
	a randomly selected subset of the training dataset, but we train each of
	our weak classifiers on the full training dataset in order to reduce the
	number of randomized portions of our method, making it easier to
	reason about.

\subsection{Partitions}
	We use k-d trees to represent partition schemes, where each level in the tree
	represents a set of cuts along a certain dimension, and we generate cuts
  in a round robin manner over dimensions $(x, y, t)$ across levels in the
  tree.
  Cuts for child nodes are generated independently, and each cut is axis-aligned
  (we incorporate random shifts, but not random rotations).
	Initially, all randomized partitions were computed according to a uniform
	distribution. However, in an attempt to avoid generating partition schemes
	that are not sufficiently discriminative, we bias the partition generation
	step according to computed distributions of active object locations across
	training data. 

	\begin{figure}[t]
		\begin{center}
			%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
			  \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/active_obj_distr_x.png}
			  \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/active_obj_distr_y.png}
			  \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/active_obj_distr_z.png}
		\end{center}
		   \caption{Histograms of counts of active objects across all 3 dimensions}
				\label{fig:long}
				\label{fig:onecol}
	\end{figure}
	
	From figure 2 we see that active objects tend to occur in the lower center
	of the field of view, and that active objects are nearly uniformly
	distributed across the temporal dimension. This is as expected, because
	the active objects are close to the hands which are in the lower field of
	view from an egocentric perspective. When generating a biased
	partition, we can choose to prefer cutting around regions that tend to
	contain active objects (denoted as bias type 2), or we can choose to prefer 
	cutting through regions that tend to contain active objects (denoted as bias type 3). 
	We denote by bias type 1 the method of using completely uniform
	distributions to generate partitions. For biased partitions, we generate
  the first cut along each dimension according to a weighted distribution
  corresponding to the observed active object regions in the training data,
  and we generate all subsequent child cuts using a uniform distribution.
  

  \begin{figure}[t]
    \begin{center}
		 \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/lvl3.png}
    \end{center}
		   \caption{An example biased (type 3) partitioning scheme corresponding to a 3-level
       pyramid}
			\label{fig:long}
			\label{fig:onecol}
  \end{figure}
	
   To represent a video clip as a randomized spatio-temporal pyramid (RSTP)
   using a particular partition scheme we use the output of object detectors
   trained in \cite{Ramanan12}, which gives bounding boxes and object
   labels. We compute histograms for each individual level in the pyramid,
   where level 0 is the entire video clip volume and level $i$ is all the
   cells of depth $i$ in the k-d tree. Note that level $i$ has $8^i$ leaf
   cells. To form the final RSTP representation, we simply concatenate the
   histograms computed for each level to form a single feature vector.
\section{Results}
	The ADL dataset has been modified since the publication of
	\cite{Ramanan12}; because of this, running the published code gives
	slightly lower accuracy than the originally published numbers. We use the
  dataset available from the authors webpage at the time of writing to
  benchmark our method.

	
	\begin{table}
		\begin{center}
			\begin{tabular}{|l|c|c|c|c|}
				\hline
        Feature Type & BoW & Temporal Pyramid & RSTP \\
				\hline\hline
        % TODO put this back after i have the results
       O & 26.6 & 29.0 & 32.7\\
        \hline
      AO & 34.9 & 36.9 & 37.9\\
				\hline
			\end{tabular}
		\end{center}
		\caption{Overall classification accuracy on pre-segmented video clips.}
	\end{table}
	
  Table 1 shows a comparison of overall classification accuracy between our
  method and two methods presented in \cite{Ramanan12}. The temporal pyramid
  has two levels, formed by making a single cut along the temporal
  dimension.
  Row 1 shows results
  obtained using only passive detected objects, while row 2 shows results obtained
  using both active (being interacted with) and passive detected objects.
  The consideration of active objects when constructing feature vectors
  gives a significant improvement over just considering passive objects, and
  in both cases our method improves on the current state of the art.

	The results shown in Table 1 are computed using a form of cross
	validation (use the video clips from person $i$ as a held out validation set, and
	train on the video clips from the remaining people).
  
  Feature vectors are computed using detections for both active and passive
  objects. The results for bag of words and temporal pyramids (2 level, with
  a single cut along the temporal dimension) are both presented in \cite{Ramanan12}.
  
  For this experiment we used a pool consisting of 100 4-level partitioning schemes. 
  Each paritioning scheme was of bias type 3, meaning the cuts for level 1
  were biased such that they had a tendency to cut through regions containing
  active objects, and the cuts for levels 2 and 3 were drawn from a uniform
  distribution. The work of \cite{Jiang12}, which uses a similar
  pyramid-based boosting approach for 2D image recognition, found that using
  pyramids with more than 3 levels actually led to a decrease in overall
  accuracy due to oversegmentation of images. However, we found that in the
  3D case 4-level pyramids give better overall accuracy than coarser-grained
  representations.
	
  \begin{figure}[t]
		\begin{center}
			%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
			  \includegraphics[width=1.0\linewidth]{/u/tomas/thesis/figures/confn.png}
		\end{center}
		   \caption{Confusion matrix for our method using active objects}
				\label{fig:long}
				\label{fig:onecol}
	\end{figure}
	As seen in the confusion matrix, our method has particularly good
  performance for activity types 1 and 6 (``combing hair'' and ``drying
  hands/face'', respectively). Some activity types on which our method does
  poorly are 10 and 11, which are ``making tea'' and ``making coffee'',
  respectively. Since the two activity types are similar it is not
  unexpected that a recognition system would confuse them often.
  
  % show the impact of bias (vp)
  \textit{TODO: plot accuracy vs pool size for each bias type} 
  
  \textit{TODO: accuracy on GA tech egocentric dataset} 
  % which partitions got selected
  
  To support our claim that 4-level biased pyramids tend to be most
  discriminative, we created a heterogenous pool containing partitions of
  different types. Specifically, the heterogenous pool contains 30 2-level
  partitions of each bias type, 30 3-level partitions of each bias type, and
  30 4-level partitions of each bias type, for a total of 270 partitions.
  We generated 20 random 50/50 train/test splits, fixed the number of
  boosting rounds to 5, and observed which types of partition were most
  often selected.
  We found that 3-level pyramids are often preferred to 2-level
	pyramids, and 4-level pyramids are often preferred to 3-level pyramids. 
  Specifically, we found that 2-level pyramids of bias type 3 were selected
  21\% of the time, 3-level pyramids of bias type 3 were selected 19\% of
  the time, and 4-level pyramids of bias type 3 were selected 37\% of the
  time. 2-level and 4-level unbiased pyramids were never selected. Thus,
  biased partition schemes that cut through regions that tend to contain
  active objects are clearly more discriminative than other types of
  paritition schemes, especially those which are unbiased.
	
\section{Conclusion and Future Work}
	We have presented an application of the well-known boosting framework
	with results that improve upon the current state of the art. 
	Our main novel contribution is a method for generating biased partition
	schemes.
	Future work could incorporate different types of biases when generating
	partitions. The ADL dataset also includes annotations for hand positions,
	which we have incorporated implicitly through our generation of partitions
	biased relative to regions which tend to contain active objects. However,
	it could be possible to incorporate explicit information given by hand
	positions to obtain better classification results.
	Additionally, it may be worthwhile to investigate the performance of other
	variants of the boosting algorithm.
	The partitions we focus on contain cuts that are
  planar and axis-aligned, but it is also possible to carve up the
	video volume in non-linear ways. Such a method would involve more
	sophisticated computational geometry, but may yield a more discriminative
	partitioning scheme that could lead to better classification accuracy.
  

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
